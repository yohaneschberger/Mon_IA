# ğŸ¤– Expert DevOps IA - Stack Locale & Persistante

Ce projet est un assistant intelligent spÃ©cialisÃ© dans l'ingÃ©nierie DevOps. Contrairement aux solutions Cloud, il offre une **confidentialitÃ© totale** et une **mÃ©moire Ã  long terme** grÃ¢ce Ã  une architecture locale optimisÃ©e pour les GPU NVIDIA.

---

## ğŸ“¸ AperÃ§u de l'application

### ğŸ–¥ï¸ AperÃ§u du fonctionnement
![DÃ©mo de l'expert DevOps](./images/demo.png)

---

## ğŸ—ï¸ Architecture du SystÃ¨me

Le projet repose sur une orchestration de micro-services conteneurisÃ©s permettant une sÃ©paration nette des responsabilitÃ©s :

| Service | Technologie | RÃ´le |
| :--- | :--- | :--- |
| **Frontend** | Streamlit | Interface chat intuitive, fluide et rÃ©active. |
| **Backend** | FastAPI | Orchestration, gestion du streaming et logique mÃ©tier. |
| **LLM Engine** | Ollama | HÃ©bergement du modÃ¨le **Llama 3** avec support GPU (CUDA). |
| **Database** | Redis | Stockage persistant de l'historique des conversations. |
| **DB UI** | RedisInsight | Interface visuelle pour explorer la mÃ©moire de l'IA. |
| **Monitoring** | Glances | Dashboard de surveillance des ressources (CPU/GPU/RAM). |

---

## ğŸŒŸ FonctionnalitÃ©s ClÃ©s

* **ğŸ§  MÃ©moire Persistante** : GrÃ¢ce Ã  Redis, l'IA conserve le contexte de vos Ã©changes mÃªme aprÃ¨s un redÃ©marrage des conteneurs.
* **âš¡ InfÃ©rence GPU AccÃ©lÃ©rÃ©e** : OptimisÃ© pour les cartes NVIDIA (testÃ© sur RTX 3050) pour une gÃ©nÃ©ration de texte quasi instantanÃ©e.
* **ğŸ”„ Hot Reload (Dev Mode)** : DÃ©veloppement fluide grÃ¢ce aux volumes Docker. Modifiez le code dans `app/`, l'application se met Ã  jour sans `rebuild`.
* **ğŸ”’ 100% Hors-Ligne** : Aucune donnÃ©e ne quitte votre machine. IdÃ©al pour manipuler des fichiers de configuration sensibles.

---

## ğŸ› ï¸ Stack Technique

* **Langages** : Python 3.10+
* **Frameworks** : FastAPI, Streamlit
* **Base de donnÃ©es** : Redis Stack
* **Infrastructure** : Docker Compose
* **ModÃ¨le LLM** : Llama 3 (via Ollama)
* **AccÃ©lÃ©ration** : NVIDIA Container Toolkit (CUDA)

---

## ğŸš€ Installation et DÃ©marrage

### 1. PrÃ©requis
* Docker et Docker Compose installÃ©s.
* Drivers NVIDIA et **NVIDIA Container Toolkit** configurÃ©s.

### 2. Lancement de la Stack
```bash
# Clonez le dÃ©pÃ´t
git clone [https://github.com/votre-utilisateur/votre-projet.git](https://github.com/votre-utilisateur/votre-projet.git)
cd votre-projet

# Lancer les services (le premier lancement build les images)
sudo docker compose up -d
```

### 3. AccÃ¨s aux interfaces
* ğŸ’¬ Chat IA : http://localhost:8501
* ğŸ“Š Monitoring (Glances) : http://localhost:61208
* ğŸ” Exploration Redis : http://localhost:5540

---

## ğŸ“ Structure du Projet

.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # API Backend (FastAPI)
â”‚   â”œâ”€â”€ ui.py            # Interface Utilisateur (Streamlit)
â”‚   â””â”€â”€ memory/          # Backup local de la mÃ©moire
â”œâ”€â”€ docker-compose.yml   # Orchestration des micro-services
â”œâ”€â”€ Dockerfile           # Configuration de l'image Python
â””â”€â”€ README.md            # Documentation

---

## ğŸ’¡ Astuces DÃ©veloppement

**Hot Reload** : Le projet utilise des Bind Mounts. Si vous modifiez app/main.py ou app/ui.py, le serveur se recharge automatiquement. Inutile de relancer docker compose up --build.

**Nettoyage de la mÃ©moire** : Pour rÃ©initialiser les discussions, vous pouvez vider les clÃ©s dans RedisInsight ou utiliser la commande :
```bash
docker exec -it <nom_du_conteneur_redis> redis-cli FLUSHALL
```