# üöÄ Expert DevOps IA - Instance Locale

Ce projet est un assistant intelligent sp√©cialis√© dans le diagnostic et la r√©solution de probl√®mes DevOps. Il utilise un mod√®le de langage (LLM) performant tournant localement pour garantir la confidentialit√© des donn√©es et une disponibilit√© totale sans frais d'API.

## üèóÔ∏è Architecture du Projet

Le projet repose sur une architecture en micro-services conteneuris√©e :
* **Interface (Frontend)** : D√©velopp√©e avec **Streamlit** pour une interaction utilisateur fluide et intuitive.
* **Cerveau (Backend)** : Une API **FastAPI** qui orchestre la logique et formate les requ√™tes.
* **Moteur IA** : **Ollama** h√©bergeant le mod√®le **Llama 3**, avec acc√©l√©ration mat√©rielle via GPU NVIDIA.



---

## üõ†Ô∏è Stack Technique

* **Langage** : Python 3.10+
* **Framework API** : FastAPI / Uvicorn
* **Interface Web** : Streamlit
* **Conteneurisation** : Docker & Docker-Compose
* **Mod√®le IA** : Llama 3 (via Ollama)
* **Acc√©l√©ration** : NVIDIA CUDA Support (RTX 3050)

---

## üõ†Ô∏è Fonctionnalit√©s Cl√©s

* **Inf√©rence 100% Locale** : Confidentialit√© totale, aucune donn√©e n'est envoy√©e dans le cloud.
* **Acc√©l√©ration Hardware** : Optimis√© pour les GPU NVIDIA (test√© sur RTX 3050) via `nvidia-container-runtime`.
* **R√©ponses en Fran√ßais** : Syst√®me configur√© pour traduire les concepts complexes en fran√ßais technique clair.
* **Haute Disponibilit√©** : Configuration `restart: always` pour un service op√©rationnel d√®s le d√©marrage du PC.
* **Persistance** : Volume Docker d√©di√© pour conserver les mod√®les et √©viter les ret√©l√©chargements.
---

## üöÄ Exemple de Requ√™te & R√©sultat
**Utilisateur :** *"Pourquoi mon `docker-compose up` √©choue avec 'port already allocated' ?"*

**R√©ponse de l'Expert (IA) :**
1. **Indices** : Port 8080 occup√©, service Nginx en conflit.
2. **Lien** : Une instance de test tourne d√©j√† en arri√®re-plan sur le m√™me port.
3. **Solution** : `docker ps` pour identifier l'ID, puis `docker stop <ID>` ou changer le mapping de port dans le fichier YAML.

---
## üöÄ Installation et D√©marrage

### Pr√©requis
* Docker et Docker-Compose install√©s.
* NVIDIA Container Toolkit (pour l'acc√©l√©ration GPU).


# Lancer l'infrastructure
docker-compose up -d

# Acc√©der √† l'interface
# http://localhost:8501

### Lancement
1. Clonez le d√©p√¥t :
   ```bash
   git clone [https://github.com/votre-utilisateur/votre-projet.git](https://github.com/votre-utilisateur/votre-projet.git)
   cd votre-projet
   ```
2. Lancer l'infrastructure :
   ```bash
   docker-compose up -d
   ```
3. T√©l√©charger le mod√®le d'IA (uniquement la premi√®re fois) :
   ```bash
   docker exec -it mon_ia_ollama_1 ollama pull llama3
   ```
4. Acc√©der √† l'interface : Ouvrez votre navigateur et allez √† l'adresse suivante : üëâ   http://localhost:8501
