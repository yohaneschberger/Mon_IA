# üöÄ Expert DevOps IA - Instance Locale

Ce projet est un assistant intelligent sp√©cialis√© dans le diagnostic et la r√©solution de probl√®mes DevOps. Il utilise un mod√®le de langage (LLM) performant tournant localement pour garantir la confidentialit√© des donn√©es et une disponibilit√© totale sans frais d'API.

## üèóÔ∏è Architecture du Projet

Le projet repose sur une architecture en micro-services conteneuris√©e :
* **Interface (Frontend)** : D√©velopp√©e avec **Streamlit** pour une interaction utilisateur fluide et intuitive.
* **Cerveau (Backend)** : Une API **FastAPI** qui orchestre la logique et formate les requ√™tes.
* **Moteur IA** : **Ollama** h√©bergeant le mod√®le **Llama 3**, avec acc√©l√©ration mat√©rielle via GPU NVIDIA.



---

## üõ†Ô∏è Stack Technique

* **Langage** : Python 3.10+
* **Framework API** : FastAPI / Uvicorn
* **Interface Web** : Streamlit
* **Conteneurisation** : Docker & Docker-Compose
* **Mod√®le IA** : Llama 3 (via Ollama)
* **Acc√©l√©ration** : NVIDIA CUDA Support (RTX 3050)

---

## üöÄ Installation et D√©marrage

### Pr√©requis
* Docker et Docker-Compose install√©s.
* NVIDIA Container Toolkit (pour l'acc√©l√©ration GPU).

### Lancement
1. Clonez le d√©p√¥t :
   ```bash
   git clone [https://github.com/votre-utilisateur/votre-projet.git](https://github.com/votre-utilisateur/votre-projet.git)
   cd votre-projet
